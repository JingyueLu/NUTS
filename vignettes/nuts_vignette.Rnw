\documentclass{article}

\usepackage{amsmath, amsthm, amssymb}
\usepackage[round]{natbib}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage[margin=1in]{geometry}
\usepackage{color}


% The line below tells R to use knitr on this.
%\VignetteEngine{knitr::knitr}

\title{The Hamiltonian Monte Carlo and the No-U-Turn Sampler}
\author{Jingyue Lu \and Marco Palma}

\begin{document}
\SweaveOpts{concordance=TRUE}
%\SweaveOpts{concordance=TRUE}
%\SweaveOpts{concordance=TRUE}

\maketitle

\begin{abstract}
We present the R package `NUTS', which contains functions for Hamiltonian Monte Carlo and one of its extensions called No-U-Turn Sampler with Dual Averaging.
\end{abstract}

\section{Introduction}

This project investigates the No-U-Turn Sampler (NUTS). M.D. Hoffman and A. Gelman introduced NUTS in 2011 to address the tuning issues involved in Hamiltonian Monte Carlo (HMC) algorithm.
%Using Hamiltonian Dynamics, HMC avoids simple random work behaviour by proposing distant proposals for Metropolis algorithm, thereby converging to high-dimensional target distributions at a much faster speed than other general methods.
%The performance of HMC depends heavily on two tuning parameters
%: the trajectory length and the stepsize.
%and a poor choice of these two parameters will lead to drastic decrease of the performance of HMC. NUTS is designed to specifically address the problem of tuning the trajectory length.
To be more specific, we start by introducing the theoretical ideas behind HMC and NUTS. Then we %A short theoretical foundation for both algorithms is also included. Both HMC and NUTS are
implement both methods in R to compare their performances. Especially, we are interested in how effective NUTS is as an extension of HMC.

\section{Hamiltonian Monte Carlo}

The Hamiltonian Monte Carlo (also known as hybrid Monte Carlo, introduced by \citealp{duane1987hybrid}) is a Markov chain Monte Carlo method that avoids simple random work behaviour by proposing remote states for Metropolis algorithm, thereby achieving rapid convergence to a target distribution.
%converging to high-dimensional target distributions at a much faster speed than other general methods.%that overcomes the random walk behaviour typical of this class of algorithms in order to achieve faster convergence to a target distribution.
% This is obtained by introducing for each model variable $\theta_d$ an auxiliary variable $r_d$ called momentum, whose distribution is easy to sample from (usually a standard normal distribution).
The idea of HMC is to introduce a $d$-dimensional vector $r$ called momentum (where $d$ is the dimension of the parameter vector $\theta$), independently drawn from a distribution we can easily sample from (usually a standard multivariate normal). The unnormalized joint density can be written as
$$ p(\theta,r) \propto \exp\left\{\mathcal{L}(\theta)-\frac{1}{2} r \cdot r\right\} $$
where $\mathcal{L}$ is the unnormalized logarithm of the joint posterior density of $\{\theta_k\}$ and $(\cdot)$ is an inner product.

\par Samples of $(\theta,r)$ are obtained by using the St{\"o}rmer-Verlet \textit{leapfrog integrator}. Given the gradient $\nabla_{\theta}\mathcal{L}$ and the step size $\epsilon$, the updates proceed as follows:
\begin{equation*}
r^{t+\epsilon/2}=r^t+\epsilon/2 \nabla_{\theta}\mathcal{L}(\theta^t)
\quad \theta^{t+\epsilon} = \theta^t+\epsilon r^{t+\epsilon/2} \quad
r^{t+\epsilon}=r^{t+\epsilon/2}+(\epsilon/2)\nabla_{\theta}\mathcal{L}(\theta^{t+\epsilon}).
\end{equation*}

\par The leapfrog procedure is applied $L$ times in order to generate a pair $(\tilde{\theta},\tilde{r})$. Then the proposal for the $m$-th iteration of the Markov chain is $(\tilde{\theta},-\tilde{r})$ (where the minus sign for $r$ is only used to guarantee time reversibility). According to Metropolis ratio, the proposal is accepted %using a standard Metropolis accept-reject procedure
with a probability

\begin{equation*}
\alpha=\min\left\{1,\frac{p(\tilde{\theta},\tilde{r})}{p(\theta^{m-1},r^{*})}\right\}
\end{equation*}
\noindent where $r^{*}$ is the resampled momentum at the $m$-th iteration.

Despite the increase in efficiency, the usability and the performances of HMC are affected not only by the computation of $\nabla_{\theta}\mathcal{L}$ (that can be addressed via numerical procedures) but also by the step size $\epsilon$ and number of steps $L$ chosen within the leapfrog. Indeed, when $\epsilon$ is too large the acceptance rates will be low, whereas for small values of $\epsilon$ there will be a waste of computation because of the tiny steps. In terms of $L$, if it is too small the samples will be close to each other, but if it is too large the trajectory in the parameters space will loop back to the previous steps.

\section{No-U-Turn Sampler}

%In this section, we introduce NUTS to address the problem of tuning the trajectory length $L$. NUTS is a method that builds upon HMC. Unlike HMC, which sets a fixed trajectory length $L$ for all proposals, NUTS dynamically chooses $L$ for each proposal using the idea of No-U-Turn.

NUTS addresses the problem of tuning the number of steps $L$. In short, NUTS repeatedly doubles the length of the current trajectory until increasing $L$ no longer leads to an increased distance between the initial $\theta$ and a newly proposed $\tilde{\theta}$. That is, the $\tilde{\theta}$ makes a "U-turn".

\subsection{Derivation of simplified NUTS algorithm}
The derivation of NUTS algorithm can be divided into two parts: the conditions this algorithm has to satisfy in order to be theoretically sound and the criteria NUTS uses to stop the doubling procedure.

To simplify the derivation of NUTS, Hoffman and Gelman introduced a slice variable $u$. Simplified NUTS considers the augmented model

$$p(\theta,r,u)\propto\mathbb{I}\left[u\in\left[0,\exp\left\{\mathcal{L}(\theta)-\frac{1}{2}r\cdot r\right\}\right]\right],$$

where $\mathbb{I}[\cdot]$ is 1 if $u\in \left[0,\exp\{\mathcal{L}(\theta)-\frac{1}{2}r\cdot r\}\right]$ is true and 0 otherwise.
%The useful results of introducing a slice variable $u$ are that the conditional probabilities $p(u|\theta,r) \sim \text{Unif}(u;[0, \exp\{\mathcal{L}(\theta)-\frac{1}{2}r\cdot r\}])$ and $p(\theta,r|u) \sim \text{Unif}(\theta', r' | \exp\{\mathcal{L}(\theta)-\frac{1}{2}r\cdot r\}\geq u)$ are both uniform and hence can be easily simulated
Introducing $u$ renders the conditional probabilities $p(u|\theta,r) \sim \text{Unif}(u;[0, \exp\{\mathcal{L}(\theta)-\frac{1}{2}r\cdot r\}])$ and $p(\theta,r|u) \sim \text{Unif}(\theta', r' | \exp\{\mathcal{L}(\theta)-\frac{1}{2}r\cdot r\}\geq u)$ and hence simplifies the simulation.

\par In terms of theoretical requirements, the simplified NUTS algorithm must not only leave the target distribution invariant but also guarantee the time reversibility. Under several mild conditions, NUTS uses the following procedure to sample $\theta^{t+1}$ from $\theta^{t}$ to achieve invariant target distribution:
\begin{enumerate}
\item sample $r\sim\mathcal{N}(0,I)$,
\item sample $u\sim\text{Unif}\left(\left[0,\exp\left\{\mathcal{L}(\theta^t)-\frac{1}{2}r\cdot{r}\right\}\right]\right)$,
\item sample $\mathcal{B}$, $\mathcal{C}$ from their conditional distribution $p(\mathcal{B},\mathcal{C}|\theta^t,r,u,\epsilon)$,
\item sample $\left(\theta^{t+1},r \right)$ uniformly from the set $\mathcal{C}$.
\end{enumerate}
\noindent Here, $\mathcal{C}$ is a set of candidate position-momentum states while $\mathcal{B}$ is the set of all position-momentum states computed by leapfrog integrator during each NUTS iteration. Clearly, $\mathcal{C}\subseteq\mathcal{B}$. For the purpose of this project, we omit the proof of the validity of the procedure but only state the key observations and results. We first point out that steps 1, 2, 3 constitute a valid Gibbs sampling update for $r$, $u$, $\mathcal{B}$, $\mathcal{C}$. Secondly,
%the construction of the distribution $p(\mathcal{B},\mathcal{C}|\theta^t,r,u,\epsilon)$ relies on the strategy employed by NUTS to achieve time reversibility and will be introduced later. Thirdly and lastly,
as a result of the prequiresites of the above procedure, we use the following condition to determine whether a state in $\mathcal{B}$ is also in $\mathcal{C}$:

\begin{equation}
(\theta',r')\in\mathcal{C},\qquad \text{if}\quad u\leq \exp\left\{\mathcal{L}(\theta')-\frac{1}{2}r'\cdot r'\right\}
\label{cond1} \tag{C.1}
\end{equation}

%We now consider the time reversibility requirement. Time reversibility is important as it ensures the algorithm converge to the correct distribution. NUTS uses a recursive algorithm to preserve time reversibility.

 \par For what concerns time reversibility, it is important as it ensures the algorithm converge to the target distribution. NUTS uses a recursive algorithm to preserve it. Recall that, to find a trajectory length for each NUTS interation, NUTS doubles the current trajectory repeatedly until an U-turn is encountered. % To simulate forward and backward movement in time, during each doubling, NUTS allows the new subtrajectory to start from either the leftmost or rightmost point of the old trajectory and use leapfrog to trace a path either running backwards or forwards respectively. The proccess continues until stopping criteria are met. To illustrate, we assume the starting point is $(\theta_0^1,r)$, where the subscript is the number of step and the superscript is the index of the point in that step. Also, let $v\in\{1,-1\}$. $v$ is randomly choosen in each step to present the direction of movement. We are only interested in the path for $\theta$.
To simulate forward and backward movement in time, during each doubling, a new subtrajectory is traced by leapfrog either forward from the rightmost state or backward from the leftmost state until the stopping criteria are met. To illustrate, we assume the starting point is $(\theta_0^1,r)$, where the subscript is the number of doubling steps and the superscript is the index of the state in that doubling step. Also, let $v\in\{1,-1\}$ be randomly chosen in each doubling step to determine the direction of movement. We are interested in the path for $\theta$:\\
\textbf{Step j=1:}   (v= 1)  $ \Rightarrow \theta_0^1 \textcolor{red}{\rightarrow \theta_1^1}.$ \\
\textbf{Step j=2:}   (v= 1)  $ \Rightarrow \theta_0^1 \rightarrow \theta_1^1 \textcolor{red}{\rightarrow \theta_2^1 \rightarrow \theta_2^2}.$ \\
\textbf{Step j=3:}   (v=-1)  $ \Rightarrow \textcolor{red}{\theta_3^4\leftarrow\theta_3^3\leftarrow\theta_3^2\leftarrow \theta_3^1\leftarrow}\theta_0^1 \rightarrow \theta_1^1 \rightarrow \theta_2^1 \rightarrow \theta_2^2.$ \\
\noindent In the example above, we build a three-step path. We can also think the path after each step $j$ as a binary tree of height $j$, so final path is a binary tree of height 3
$$\underbrace{\underbrace{\underbrace{\theta_3^4\leftarrow\theta_3^3}_{\text{level }1}\leftarrow\underbrace{\theta_3^2\leftarrow \theta_3^1}_{\text{level }1}}_{\text{level }2}\leftarrow\underbrace{\underbrace{\theta_0^1 \rightarrow \theta_1^1}_{\text{level }1} \rightarrow \underbrace{\theta_2^1 \rightarrow \theta_2^2}_{\text{level }1}}_{\text{level }2}}_{\text{level }3}.$$

\par Finally, we discuss the stopping criteria used in NUTS. A straightforward and essential stopping condition for NUTS implements the idea of no-U-turn. We observe that when $\tilde{\theta}$ makes a U-turn, the following derivative
%, with respect to time, of half the squared distance (half is chosen to simplify calculations) between $\theta$ and $\tilde{\theta}$
should be less than 0:
%. Mathematically, we have
\begin{equation}
\frac{d}{dt}\frac{(\tilde{\theta}-\theta)\cdot (\tilde{\theta}-\theta)}{2} = (\tilde{\theta}-\theta)\cdot\frac{d}{dt}(\tilde{\theta}-\theta)= (\tilde{\theta}-\theta)\cdot r < 0.
\label{cond2} \tag{C.2}
\end{equation}
% Thus, the NUTS algorithm stops when:
%
% \textbf{\eqref{cond2}:}
% $$(\theta_{end}-\theta_{start})\cdot r < 0.$$
The second equality is due to a property of Hamiltonian system in physics. This condition is checked for each subtree and also for the whole tree. %Other than the stopping condition for U-turn, NUTS also stops expanding $\mathcal{B}$ when any newly discovered states in the continuing process are likely to have near 0 probability to be in $\mathcal{C}$ . To formulate, NUTS develops the following condition based on \eqref{cond1}:
In addition, NUTS also stops expanding $\mathcal{B}$ when any newly discovered states in the continuing process has extremely low probability to be in $\mathcal{C}$. To formulate, NUTS develops the following condition based on \eqref{cond1}:
% \textbf{\eqref{cond3}:}
% The algorithm stops when
\begin{equation}
\mathcal{L}(\theta) -\frac{1}{2}r\cdot r -\log u > -\Delta_{max}.
\label{cond3} \tag{C.3}
\end{equation}
\noindent In NUTS, $\Delta_{max}$ is set to 1000, so the algorithm continues as long as the simulation is moderately accurate.

\par So far, we have addressed all aspects needed for deriving the simplified NUTS algorithm. We summarise the simplified NUTS algorithm below.
\begin{algorithm}
\caption{Sample for a point $\tilde{\theta}$ using a NUTS iteration}
\begin{algorithmic}
\REQUIRE {The initial position $\theta_0^1$.}
\STATE {Resample $r\sim\mathcal{N}(0,I)$.}
\STATE {Resample $u\sim\text{Unif}([0,\exp\{\mathcal{L}(\theta^t)-\frac{1}{2}r\cdot r\}])$.}
\STATE\COMMENT {$j$ is the number of doubling steps we take to build a trajectory path.}
\STATE\COMMENT {$s$ is an indicator variable. It becomes 0 when a stopping criterion is met.}
\STATE {Initialise $j=0$, $s=1$, $\mathcal{C}=\{(\theta_0^1,r)\}$}
\STATE\COMMENT {Set the rightmost (+) and leftmost (-) points of the trajectory path.}
\STATE {Initialise $\theta^{+} =\theta_0^1$, $\theta^{-} =\theta_0^1$.}
\WHILE {s=1}
  \STATE{Build a binary tree of height $j$}
  \WHILE{Build a binary tree of height $j$}
    \STATE{ For each new node: check C.1 to determine whether or not to add the new node into $\mathcal{C}$.}
    \STATE{For each new node: check C.3. If Condition Three is met, set $s=0$.}
    \STATE{For each subtree: check C.2. If Condition Two is met, set $s=0$.}
  \ENDWHILE
  \STATE{Update $\theta^{+}$ and $\theta^{-}$ to be the rightmost and leftmost point of the whole path.}
  \STATE{Check C.2 for the whole path.If it is met, set $s=0$. }
  \STATE{j= j+1.}
\ENDWHILE
\STATE{Sample $\tilde{\theta}$ uniformly from $\mathcal{C}$.}
\end{algorithmic}
\end{algorithm}

% 1. Resample $r\sim\mathcal{N}(0,I)$.
% 2. Resample $u\sim\text{Unif}([0,\exp\{\mathcal{L}(\theta^t)-\frac{1}{2}r\cdot r\}])$.
% %j is the number of doubling steps we take to build a trajectory path.
% %s is an indicator variable. It becomes 0 when a stopping criterion is met.
% 3. Initialise $j=0$, $s=1$, $\mathcal{C}=\{(\theta_0^1,r)\}$.
% %Set the rightmost (+) and leftmost (-) points of the trajectory path.
% 4. Initialise $\theta^{+}=\theta_0^0$,$\theta^{-}=\theta_0^0$.
% 5. **while** s=1 do
%     1. Build a binary tree of height j
%         1. For each new node: check \eqref{cond1} to determine whether or not to add the new node into $\mathcal{C}$.
%         2. For each new node: check \eqref{cond3}. If \eqref{cond3} is met, set $s=0$.
%         3. For each subtree: check \eqref{cond2}. If \eqref{cond2} is met, set $s=0$.
%     2. For the newly generated whole path:
%         1. Update $\theta^{+}$ and $\theta^{-}$ to be the rightmost and leftmost point of the whole path.
%         2. Check \eqref{cond2}. If it is met, set $s=0$.
%         \# Update variables for while loop
%     3. j= j+1
% 6. Sample $\tilde{\theta}$ uniformly from $\mathcal{C}$.

\subsection{Efficient NUTS}
\par \citet{hoffman2011no} also proposed an efficient version of the NUTS algorithm. Firstly, the execution is halted exactly once a stopping criterion is met instead of at the end of the corresponding doubling step. Secondly, simplified NUTS requires to store $2^j$ states to perform uniform sampling at the end. A solution for reducing this memory requirement (from $O(2^j)$ to $O(j)$, see \citealp{hoffman2011no}) is to use a more sophisticated transition kernel and to exploit the binary tree structure of the trajectory path. We refer interested reader to \citet{hoffman2011no} for details. The NUTS function included in the package is an efficient algorithm with these improvements implemented.

% Simplified NUTS algorithm need to evaluate log posterior probability and its gradient at $2^j-1$ points apart from $O(2^j)$ operations to check the stopping criteria \citep{hoffman2011no}. Furthermore, the final doubling iteration continues even when a stopping criterion is met in the middle of the process. In terms of memory, simplifed NUTS requires to store $2^j$ states to perform uniform sampling at the end. These facts seriously deterioate the efficiency of NUTS. The second issue can be easily solved by terminating the excucation of the recrusion once $s$ becomes $0$. \citet{hoffman2011no} prosposed a solution for reducing the meomery requirment from $O(2^j)$ to $O(j)$. The key idea of this memory reduction is to use a more sophisticated transition kernal and to exploit the binary tree structure of the trajectory path. We refer interested reader to \citet{hoffman2011no} for details. The NUTS function included in the package is an efficient algorithm with these improvements implemented.

\section{Dual averaging}

In \citet{hoffman2011no} a method based on the primal-dual algorithm by \citet{nesterov2009dual} is also provided for setting the step size $\epsilon$ for both HMC and NUTS.
%, primarily intended for stochastic convex optimization.
In the context of MCMC, considered the statistic $H_t=\delta-\alpha_t$ where $\delta$ is a specified average acceptance probability and $\alpha_t$ is the observed Metropolis one at time $t$. %Suppose that there is a tunable parameter $x \in \mathbb{R}$ for which the nonincreasing function
The key idea of the dual averaging algorithm is that, under some specific conditions, $H_t=0$ can be approached (that is, to approximate the desired average acceptance probability) by tuning $\log(\epsilon)$ using an iterative procedure. In the implementation, $\epsilon$ is tuned during the warmup phase (the user needs to specify the number of iterations of the warmup phase) and kept constant for the subsequent iterations. The user needs to specify the target mean acceptance rate $\delta$ and the number of iterations of the warmup phase.

\vspace{0.5in}

\section{NUTS Package and evaluations}

We first introduce the NUTS Package and how we can use this package to generate samples from a posterior distribution, using HMC, modified versions of HMC and efficient NUTS. We will present results by sampling from a simple distribution, which is a 2-dimensional double exponential (Laplace) distribution with zero mean and unit scale parameter. The distiribution has the density
$$f(x|\mu,b) = \frac{1}{2b}\exp(-\frac{|x-\mu|}{b}),$$
with the parameter $\theta = (\mu, b) = (0,1). We ran HMC and NUTS for 5000 iterations (M=5000) after 2500 burn-in iterations (Madapt = 2500).
<<doubleEx,eval=FALSE>>=
  #Compute log posterior density and its gradient
  L <- function(x) {
  return(list(- sum(abs(x)), - sign(x)))
  }
  samples.hmc <- Hmc(theta0 = c(1,1), epsilon = 0.2, leap.nsteps = 10, L = L, M = 5000)
  samples.hmcdual <- HmcDual(theta0=c(1,1), delta = 0.65, lambda = 1.5, L = L, M = 5000, Madapt = 2500)
  samples.nuts <- NutsDual(theta0 = c(1,1), delta = 0.6, L = L, M = 5000, Madapt = 2500)
@
Depending on the method, each result is assigned with a class and its own print function. For example, for the samples generated by NUTS, we have
<<doublePrint>>=
  download.file(url="https://github.com/jodie0399/NUTS/DoubExp.RData")
  #load("https://github.com/jodie0399/NUTS/DoubExp.RData")
  print(samples.nuts)
@
To view the outputs from these three methods, we can generate for example the histogram of the observed samples for $\theta_1$.
<<doublePlots>>=
hist(samples.hmc[,1],freq=FALSE,breaks=50,col=rgb(1,0,0,0.5),ylim=c(0,0.5), xlab = "Sampled values",main = "Histograms of the first variable" )
hist(samples.hmcdual$samples[,1],freq=FALSE,breaks=50,add=T,col=rgb(0,0,1,0.5))
hist(samples.nuts$samples[,1],freq=FALSE,breaks=50,add=T,col=rgb(0,1,0,0.5))
x = seq(-5,5,len=100)
lines(x,0.5*dexp(abs(x)),col="black") #true density in black
library(rmutil)
lines(x,dlaplace(x),col="orange") #true density in black
box()
legend("topright",col = c(rgb(1,0,0,0.5),rgb(0,0,1,0.5),rgb(0,1,0,0.5)), pch = 18, legend = c("Hmc","HmcDual","NutsDual"))

@
The histograms are substantially overlapping and they are consistent with the shape of the target distribution.

\subsection{Evaluations: HMC}
\label{evalHMC}
In this section, we will use a 30-dimensional multivariate normal distribution. Since HMC is invariant to rotation and NUTS is simply an extension of HMC, the performance of each method will be the same for any 30-dimensional normal distribution, having the same squre roots of the eigenvalues of their covariance matrix. Hence, it is sufficient for us to consider a distribution with independent components. In this project, we follow the example provided by Radford Neal's blog \cite{blogpost}. Assume the target distribution is composed of 30 independent normal distributed variables $
\theta_1,\theta_2, \dots, \theta_30$, with standard deviations of 110, 100, 26 equally spaced values between 16 and 8, 1.1, 1.0 respectively. For our experiment, we used a burnin period of 2000 iterations and sampled 12500 points afterwards.

To evaluate the performances of these methods, we divide the 12500 sample points into 20 batches with 625 observations in each batch. We are interested in the variance of the mean and variances estimates of these batches. In terms of HMC methods, different values for leapfrog steps (100, 170, 200) are used.

The results for the standard HMC with dual averaging are reported in the following plots.

<<doublePlots>>=
load("https://github.com/jodie0399/NUTS/NormaltestData.RData")
par(mfrow=c(1,2))
## variance of mean estimate ()
plot(x, SubMatrixHMD1[,1],log="y", type="b",xlab ="trajectory length 100(red); 170(green); 200(blue)", ylab="variance of mean estimate", col="red",pch=16)
points(x,SubMatrixHMD2[,1], log="y", type="b",col="green", pch=16)
points(x,SubMatrixHMD3[,1], log="y",type="b", col="blue", pch=16)

## variance of variance estimate
plot(x, SubMatrixHMD1[,2],log="y", type="b",ylim = c(1e-03,1e+07),xlab ="trajectory length 100(red); 170(green); 200(blue)", ylab="variance of variance estimate", col="red",pch=16)
points(x,SubMatrixHMD2[,2], log="y",type="b", col="green", pch=16)
points(x,SubMatrixHMD3[,2], log="y", type="b",col="blue", pch=16)
@
From the plot, we see that for any leapfrog step value, the variance of the mean estimates changes drastically for different variable $\theta$, ranging approximately from 0.01 to 100. This is rather unexpected. A similar evolution is observed for the variance of the variance estimates, although at a reduced scale. According to \citet{blogpost}, this type of behaviour is caused by the fact that Hamiltonian dynamics is periordic for each variable. If the length of the period is similar to the trajectory length, the ending point will be close to the starting point, leading to poor sample quality. A possible solution to this issue is to introduce randomness when deciding the number of leapfrog steps. In the following experiment, we implemented Neal's proposal and chose a number of leapfrog step randomly from the uniform distribution between 0.9 and 1.1 times the original number of leapfrog steps.
<<doublePlotsHMCMod>>=
load("https://github.com/jodie0399/NUTS/NormaltestData.RData")
par(mfrow=c(1,2))
plot(x, SubMatrixHMD1mod[,1],log="y",ylim = c(1e-03,1e+03), type="b",xlab ="trajectory length 100(red); 170(green); 200(blue)", ylab="variance of mean estimate", col="red",pch=16)
points(x,SubMatrixHMD2mod[,1], log="y",type="b", col="green", pch=16)
points(x,SubMatrixHMD3mod[,1], log="y", type="b",col="blue", pch=16)

## variance of variance estimate (NUTS & Independent)
plot(x, SubMatrixHMD1mod[,2],log="y",ylim = c(1e-03,1e+07), type="b",xlab ="trajectory length 100(red); 170(green); 200(blue)", ylab="variance of variance estimate", col="red",pch=16)
points(x,SubMatrixHMD2mod[,2], log="y",type="b", col="green", pch=16)
points(x,SubMatrixHMD3mod[,2], log="y", type="b",col="blue", pch=16)
@
After the modification, the issue caused by the periodic behaviour of Hamiltonial dynamics is partially addressed. The variance of means still exhibit a fluatuting pattern but at a lower magnitude. For this reason, we employ the modified HMC for the following experiments.

Another issue about HMC is that the performance of HMC is largely affected by the choice of the trajectory length $\lambda$, which we will discuss in detail when comparing it with NUTS.

\subsection{Evaluations: HMC vs. NUTS}
Evaluations of NUTS are conducted based on the approach proposed by \citet{hoffman2011no}. The first experiment is aimed at assessing whether the implementation of dual averaging is effective. The criterion used is the discrepancy between the desired average acceptance rate $\delta$ and the mean of the acceptance rate $\alpha$. For each targer distribution, we implemented NUTS and modified HMC with Dual Averaging to generate 2000 samples after a 1000 iteration burn-in period. The experiment is replicated for 15 values (NUTS) and 8 values (HMC) of $\delta$ equally spaced between 0.25 and 0.95. In terms of the trajectory length for HMC, we tried 4 different values, each of which is 1.5 times larger than the previous one. For each combination of $\delta$ and $\lambda$, 10 simulations with different random seeds have been produced. Assessments are done for both the double exponential distribution and the 30-dimensional normal distribution (introduced previously).

<<DualAvNorm30>>=
load("https://github.com/jodie0399/NUTS/Normal30_HMCDualMod.RData")
finalplot.dualav <- as.data.frame(rbind(plotmatNUTS.norm30$DualAvMatrix,plotmatHMCDualMod.norm30$DualAvMatrix))
finalplot.ESS <- as.data.frame(rbind(plotmatNUTS.norm30$EssMatrix,plotmatHMCDualMod.norm30$EssMatrix))

library(ggplot2)
sp <- ggplot(finalplot.dualav, aes(x=Delta, y=Discrepancy)) + ylim(-0.1,0.2) + geom_point(shape=1)
sp <- sp + facet_grid( ~ Model)
sp <- sp + stat_summary(fun.y=mean, colour="red", geom="line")
sp <- sp + theme_bw()
sp <- sp + ggtitle("30D Normal") + theme(plot.title = element_text(hjust = 0.5))
sp <- sp + labs(x = expression("Target acceptance rate statistics "*delta), y = expression("h - "*delta))
sp
@


<<DualAvDoubExp>>=
load("https://github.com/jodie0399/NUTS/Performances.RData")
finalplot.dualav <- as.data.frame(rbind(plotmatNUTS.doubexp$DualAvMatrix,plotmatHMC.doubexpSMALL$DualAvMatrix,plotmatHMC.doubexp$DualAvMatrix))[-c(151:310,551:630,711:790),]
finalplot.ESS <- as.data.frame(rbind(plotmatNUTS.doubexp$EssMatrix,plotmatHMC.doubexpSMALL$EssMatrix,plotmatHMC.doubexp$EssMatrix))[-c(151:310,551:710),]



library(ggplot2)
sp <- ggplot(finalplot.dualav, aes(x=Delta, y=Discrepancy)) + ylim(-0.1,0.2) + geom_point(shape=1)
sp <- sp + facet_grid( ~ Model)
sp <- sp + stat_summary(fun.y=mean, colour="red", geom="line")
sp <- sp + theme_bw()
sp <- sp + ggtitle("Double Exponential") + theme(plot.title = element_text(hjust = 0.5))
sp <- sp + labs(x = expression("Target acceptance rate statistics "*delta), y = expression("h - "*delta))
sp
@

The analysis partly confirms the findings in the original paper. For small values of $\lambda$ in HMC, the average acceptance rate is close to the desired $\delta$ for all $\delta$ tested. When the trajectory length increases, the difference between the average acceptance rates in HMC and the specified $\delta$ increases as well. It is difficult to acheive the desired acceptance rate with relatetively large $\lambda$. For what concerns NUTS, the dual averaging performances are rather unsatisfactory. Instead of a line closing to 0, the line of difference fluctuates with majority points far from 0. In addition, we notice that the variance of the discrepancies are large for high value of $\lambda$, which might be caused by the fact that HMC is highly sensitive to the trajectory length.

\par\citet{hoffman2011no} also computed the effective sample size (ESS) to show the effectiveness of NUTS. However, the formula for ESS reported in the paper has been shown to be incorrect because it does not take into account negative autocorrelations which in HMC are quite possible. For this reason we used a standard ESS function from the package "mcmcse".
<<ESSNorm30>>=
load("https://github.com/jodie0399/NUTS/Normal30_HMCDualMod.RData")
finalplot.ESS <- as.data.frame(rbind(plotmatNUTS.norm30$EssMatrix,plotmatHMCDualMod.norm30$EssMatrix))

library(ggplot2)
sp <- ggplot(finalplot.dualav, aes(x=Delta, y=Discrepancy)) + ylim(-0.1,0.2) + geom_point(shape=1)
sp <- sp + facet_grid( ~ Model)
sp <- sp + stat_summary(fun.y=mean, colour="red", geom="line")
sp <- sp + theme_bw()
sp <- sp + ggtitle("30D Normal") + theme(plot.title = element_text(hjust = 0.5))
sp <- sp + labs(x = expression("Target acceptance rate statistics "*delta), y = expression("h - "*delta))
sp
@

<<ESSDoubExp>>=
load("https://github.com/jodie0399/NUTS/Performances.RData")
finalplot.ESS <- as.data.frame(rbind(plotmatNUTS.doubexp$EssMatrix,plotmatHMC.doubexpSMALL$EssMatrix,plotmatHMC.doubexp$EssMatrix))[-c(151:310,551:710),]

library(ggplot2)
spESS <- ggplot(finalplot.ESS, aes(x=Delta, y=ESS)) + ylim(0,2000) + geom_point(shape=1)
spESS <- spESS + facet_grid( ~ Model)
spESS <- spESS + stat_summary(fun.y=mean, colour="red", geom="line")
spESS <- spESS + theme_bw()
spESS <- spESS + ggtitle("Double Exponential") + theme(plot.title = element_text(hjust = 0.5))
spESS <- spESS + labs(x = expression("Target acceptance rate statistics "*delta), y = "ESS")
spESS
@

For what concerns the normal case, the ESS for HMC is extremely small for the first four trajectory length considered but an increasing trend of ESS can be observed when trajectory length increases. We expect high trajectory length would be a better option for the 30-dimensional normal distribution. The ESS for NUTS also shows a peculiar pattern with ESS number increases again after $\delta >0.8$. With these unexpected results, we propose further analysis to be made and more experiments with much larger trajectory lengths to be carried on.
The double exponential scenario is instead close to our expectations. For HMC, our results confirmed the statement made by \cite{hoffman2011no} that HMC performs better at the optimal value $\delta=0.65$. Moreover, from the simple visualization of ESS, NUTS performed similar to HMC but not better. Still, further studies should be done.

\par Finally, we conduct an investigation into the possible premature U-turn behaviour of NUTS. This defect of NUTS was first discovered and discussed in \citet{blogpost}. In plain words, premature U-turn happens when some directions are much more constrained with respect to some other directions. Therefore, the trajectory of intermediate constrained direction reverses long before the least constrained direction is fully explored, which means NUTS stop long before achieving the optimal trajectory length. To study this behaviour, we consider again the 30-dimensinal normal distribution used in section \ref{evalHMC}. Results are shown in the following plots. An independent sample using built in function rnorm is also included.

<<Premature>>=
load("https://github.com/jodie0399/NUTS/NormalTestData.RData")

par(mfrow=c(2,2))
# variance of mean estimate (NUTS & Independent)
plot(x, SubMatrixNuts[,1],log="y", ylim = c(1e-03,1e+03), xlab ="NUTS(red) / Independent(black)", ylab="variance of mean estimate", col="red",pch=16,type="b")
points(x,SubMatrixInd[,1], log="y", col="black", pch=16,type="b")
## variance of mean estimate ()
plot(x, SubMatrixHMD1mod[,1],log="y",ylim = c(1e-03,1e+03), type="b",xlab ="trajectory length 100(red); 170(green); 200(blue)", ylab="variance of mean estimate", col="red",pch=16)
points(x,SubMatrixHMD2mod[,1], log="y",type="b", col="green", pch=16)
points(x,SubMatrixHMD3mod[,1], log="y", type="b",col="blue", pch=16)


# variance of variance estimate (NUTS & Independent)
plot(x, SubMatrixNuts[,2], ylim = c(1e-03,1e+07),log="y", xlab ="NUTS(red) / Independent(black)", ylab="variance of variance estimate", col="red",pch=16, type="b")
points(x,SubMatrixInd[,2], log="y", col="black", pch=16, type="b")

## variance of variance estimate
plot(x, SubMatrixHMD1mod[,2],log="y",ylim = c(1e-03,1e+07), type="b",xlab ="trajectory length 100(red); 170(green); 200(blue)", ylab="variance of mean estimate", col="red",pch=16)
points(x,SubMatrixHMD2mod[,2], log="y",type="b", col="green", pch=16)
points(x,SubMatrixHMD3mod[,2], log="y", type="b",col="blue", pch=16)
@
We see that the NUTS is not as efficient as HMC (by a factor of 2) in estimating variables with large standard deviation. On the contrary, NUTS outperforms HMC for variables with relatively small standard deviations. This inefficiency of NUTS in sampling variables of large standard deviation is consistent with the pre-mature U-turn behaviour suggested by \citet{blogpost}. The better performance of NUTS for variables with small standard deviations may be due to its automatic tuning procedure.

\section{Conclusion}

In this report, we have discussed some features about the No-U-Turn Sampler in comparison with standard and modified versions of HMC. In general, some of the results presented in \citet{hoffman2011no} are not confirmed by our analysis, especially for what concerns the claimed advantages of NUTS over the other HMC algorithms. We acknowledged the limits of our studies and some experiments results should be revised. The study is inconclusive regarding the effectiveness of NUTS. Further investigations should be carried out.

% now generate the bibliography from file mybib.bib

\bibliographystyle{plainnat}
\bibliography{mybib}

\end{document}
